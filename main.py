from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List
from enum import Enum
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import torch
import threading
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    hugging_face_token: str = "suka"


settings = Settings()  # type: ignore

app = FastAPI()

# Set up templates directory
templates = Jinja2Templates(directory="templates")

# Set up static files (for CSS and JS)
app.mount("/static", StaticFiles(directory="static"), name="static")

# Replace this with your actual Hugging Face token

# Model & tokenizer
model_name = "meta-llama/Llama-3.2-1B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=settings.hugging_face_token)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token=settings.hugging_face_token,
)


class RoleEnum(str, Enum):
    system = "system"
    user = "user"
    assistant = "assistant"


class Message(BaseModel):
    role: RoleEnum
    content: str


def chat_stream(prompt: str):
    """
    Generator that yields tokens as they are generated by the model.
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)
    generation_kwargs = dict(**inputs, streamer=streamer, max_new_tokens=500)

    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    for token in streamer:
        yield token


@app.post("/chat")
async def chat_endpoint(messages: List[Message]):
    """
    Accepts a list of conversation messages:
        [ {role: "system"|"user"|"assistant", content: "..."} ]
    Returns streaming tokens for the assistant's newest response.
    """
    # Build the prompt from all previous messages
    prompt = ""
    for msg in messages:
        if msg.role == RoleEnum.system:
            # prompt += f"System: {msg.content}\n"
            prompt += "You're a helpful assistant named 'Petko Petkov', a doctor specialist ready to cure any desease!"
        elif msg.role == RoleEnum.user:
            prompt += f"User: {msg.content}\n"
        elif msg.role == RoleEnum.assistant:
            prompt += f"Assistant: {msg.content}\n"
    # The model should continue with an Assistant reply
    prompt += "Assistant:"

    return StreamingResponse(chat_stream(prompt), media_type="text/plain")


@app.get("/")
async def chat_ui(request: Request):
    """
    Serve a modern chat UI using Jinja2 templates
    """
    return templates.TemplateResponse("chat.html", {"request": request})
